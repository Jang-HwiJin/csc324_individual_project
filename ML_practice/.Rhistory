dataset
y
x
x <- dataset[,1:4]
y <- dataset[,5]
x
y
dataset
x
y
y <- dataset[5]
y
y <- dataset[,5]
y
dataset
y <- dataset[5]
y
y <- dataset[,5]
y
dataset
dataset
y
dataset
x <- dataset[,1:4]
y <- dataset[,5]
y
x
# split input and output
x <- dataset[,1:4]
x
dataset
library(caret)
data(risi)
dataset <- iris
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
# dimensions of dataset
dim(dataset) #120 5, 120 instances(rows) and 5 attributes(columns)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
#The class variable is a factor. A factor is a class that has
#multiple class labels or levels.
levels(dataset$Species)
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# split input and output
x <- dataset[,1:4]
y <- dataset[,5]
data(iris)
dataset <- iris
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
# dimensions of dataset
dim(dataset) #120 5, 120 instances(rows) and 5 attributes(columns)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
#The class variable is a factor. A factor is a class that has
#multiple class labels or levels.
levels(dataset$Species)
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# split input and output
x <- dataset[,1:4]
y <- dataset[,5]
x
y
dataset
x
View(iris)
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
dataset
data(iris)
dataset <- iris
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
5
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
#The class variable is a factor. A factor is a class that has
#multiple class labels or levels.
levels(dataset$Species)
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# split input and output
x <- dataset[,1:4]
y <- dataset[,5]
x
y
dataset
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
dataset
data(iris)
dataset <- iris
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
dataset
data(iris)
dataset <- iris
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
# dimensions of dataset
dim(dataset) #120 5, 120 instances(rows) and 5 attributes(columns)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
#The class variable is a factor. A factor is a class that has
#multiple class labels or levels.
levels(dataset$Species)
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# split input and output
x <- dataset[,1:4]
y <- dataset[,5]
dataset
libray(tidyverse)
library(tidyverse)
for(i in 1:4) {
boxplot(x[,i], main=names(iris)[i])
}
par(mfrow=c(1,4))
for(i in 1:4) {
boxplot(x[,i], main=names(iris)[i])
}
?par()
mforw()
?mfrow()
# summarize attribute distributions
summary(dataset)
# barplot for class breakdown
plot(y)
y <- dataset[,5]
# barplot for class breakdown
plot(y)
# barplot for class breakdown
plot(y)
library(caret)
library(tidyverse)
library(quantmod)
library(pROC)
library(TTR)
library(corrplot)
library(FSelector)
library(rJava)
library(klaR)
library(randomForest)
library(kernlab)
library(rpart)
library(dplyr)
library(shiny)
library(DT)
library(readr)
library(zoom)
library(scales)
library(plotrix)
y <- dataset[,5]
# barplot for class breakdown
plot(y)
data(iris)
dataset <- iris
# create a list of 80% of the rows in the original dataset we can use for training
set.seed(1)
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
# dimensions of dataset
dim(dataset) #120 5, 120 instances(rows) and 5 attributes(columns)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
#The class variable is a factor. A factor is a class that has
#multiple class labels or levels.
levels(dataset$Species)
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# split input and output
x <- dataset[,1:4]
y <- dataset[,5]
# boxplot for each attribute on one image
par(mfrow=c(1,4))
for(i in 1:4) {
boxplot(x[,i], main=names(iris)[i])
}
# barplot for class breakdown
plot(y)
# barplot for class breakdown
plot(y)
# scatterplot matrix
featurePlot(x=x, y=y, plot="ellipse")
#install.packages("caret")
#install.packages("elipse)
library(caret)
#install.packages("caret")
install.packages("elipse)
library(caret)
library(tidyverse)
library(quantmod)
library(pROC)
library(TTR)
library(corrplot)
library(FSelector)
library(rJava)
library(klaR)
library(randomForest)
library(kernlab)
library(rpart)
library(ellipse)
library(dplyr)
library(shiny)
library(DT)
library(readr)
library(zoom)
library(scales)
library(plotrix)
data(iris)
dataset <- iris
# create a list of 80% of the rows in the original dataset we can use for training
set.seed(1)
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
# dimensions of dataset
dim(dataset) #120 5, 120 instances(rows) and 5 attributes(columns)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
#The class variable is a factor. A factor is a class that has
#multiple class labels or levels.
levels(dataset$Species)
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# split input and output
x <- dataset[,1:4]
y <- dataset[,5]
# boxplot for each attribute on one image
par(mfrow=c(1,4))
for(i in 1:4) {
boxplot(x[,i], main=names(iris)[i])
}
# barplot for class breakdown
plot(y)
# scatterplot matrix
featurePlot(x=x, y=y, plot="ellipse")
f
#install.packages("caret")
install.packages("ellipse")
library(ellipse)
# scatterplot matrix
featurePlot(x=x, y=y, plot="ellipse")
# box and whisker plots for each attribute
featurePlot(x=x, y=y, plot="box")
featurePlot(x=x, y=y, plot="density", scales=scales)
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) linear algorithms
set.seed(7)
fit.lda <- train(Species~., data=dataset, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(Species~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(Species~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control)
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
# summarize Best Model
print(fit.lda)
# summarize Best Model
print(fit.lda$results)
# summarize Best Model
print(fit.lda)
print(fit.lda$results)
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$Species)
library(dplyr)
library(tidyverse)
library(shiny)
library(DT)
library(readr)
library(zoom)
library(scales)
library(plotrix)
#Installing the package
#install.packages("h2o")
#loading the library
library(h2o)
###############################################################################################
#Reading all the FAANG csv files and tidying it up to combine them
amazon <- read_csv("datasets/AMZN.csv")
#amazon <- mutate(amazon, Company="Amazon")
amazon$Date <- as.Date(amazon$Date,format ='%m/%d/%Y' )
amazon <- amazon %>% select(-one_of("Adj Close"))
amazon <- amazon %>% select(-one_of("Date"))
apple <- read_csv("datasets/AAPL.csv")
apple <- mutate(apple, Company="Apple")
apple$Date <- as.Date(apple$Date,format ='%m/%d/%Y' )
apple <- apple %>% select(-one_of("Adj Close"))
facebook <- read_csv("datasets/FB.csv")
facebook <- mutate(facebook, Company="Facebook")
facebook$Date <- as.Date(facebook$Date,format ='%m/%d/%Y' )
facebook <- facebook %>% select(-one_of("Adj Close"))
google <- read_csv("datasets/GOOG.csv")
google <- mutate(google, Company="Google")
google$Date <- as.Date(df$Date,format ='%m/%d/%Y' )
google <- google %>% select(-one_of("Adj Close"))
netflix <- read_csv("datasets/NFLX.csv")
netflix <- mutate(netflix, Company="Netflix")
netflix$Date <- as.Date(netflix$Date,format ='%m/%d/%Y' )
netflix <- netflix %>% select(-one_of("Adj Close"))
#Combining all the FAANG companies into one dataframe
FAANG <- rbind(amazon, apple, facebook, google, netflix)
#Removing Open, High, Low, and Adj Close column
FAANG <- FAANG %>% select(-one_of("Adj Close"))
FAANG$Date <- as.Date(FAANG$Date,format ='%m/%d/%Y' )
#Rearranging the dataframe so that Company comes first
FAANG <- select(FAANG, Company, Date, Open, High , Low, Close, Volume)
###############################################################################################
#Calculating the avergae highs of inidividual and FAANG
amazon_avg = mean(amazon$High)
amazon_avg
apple_avg = mean(apple$High)
apple_avg
facebook_avg = mean(facebook$High)
facebook_avg
google_avg = mean(google$High)
google_avg
netflix_avg = mean(netflix$High)
netflix_avg
faang_avg <- c(amazon_avg, apple_avg, facebook_avg, google_avg, netflix_avg)
labels <- c("Amazon", "Apple", "Facebook", "Google", "Netflix")
piepercent<- round(faang_avg*100/sum(faang_avg), 1)
piepercent <- percent(piepercent/100, accuracy = .1)
#Pie chart with no percents
pie(faang_avg, labels, main = "Average Highs of FAANG pie chart", col = rainbow(length(faang_avg)))
#Pie chart with percents
pie(faang_avg, labels = piepercent, main="Average Highs of FAANG pie chart", col = rainbow(length(faang_avg)))
legend("topright", labels, fill = rainbow(length(faang_avg)))
#3D pie chart
pie3D(faang_avg,labels = labels, explode = 0.1, main = "Average Highs of FAANG pie chart")
###############################################################################################
#shifting n rows up of a given variable
shift <- function(x, n) {
c(x[-(seq(n))], rep(NA, n))
}
amazon$shifted <- shift(amazon$Close, 1)
tail(amazon)
#remove NA observations
amazon <- na.omit(amazon)
write.csv(amazon, "amazon.csv")
#Initializing the Virtual Machine using all the threads (-1) and 16gb of memory
h2o.init(nthreads = -1, max_mem_size = "16g")
amazon <- h2o.importFile("amazon.csv")
h2o.describe(amazon)
y <- "shifted" #variable we want to forecast
x <- setdiff(names(amazon), y)
parts <- h2o.splitFrame(amazon, .80)
train <- parts[[1]]
test <- parts[[2]]
#Train the Model
automodel <- h2o.automl(x, y, train, test, max_runtime_secs = 120)
#Obtained a list of models in order of performance. To learn more about them just call
automodel@leader
#Apply the Mode
predictions <- h2o.predict(automodel@leader, test)
predictions
#Obtained a list of models in order of performance. To learn more about them just call
automodel@leader
predictions
library(dplyr)
library(tidyverse)
library(shiny)
library(DT)
library(readr)
library(zoom)
library(scales)
library(plotrix)
#Installing the package
#install.packages("h2o")
#loading the library
library(h2o)
###############################################################################################
#Reading all the FAANG csv files and tidying it up to combine them
amazon <- read_csv("datasets/AMZN.csv")
#amazon <- mutate(amazon, Company="Amazon")
amazon$Date <- as.Date(amazon$Date,format ='%m/%d/%Y' )
amazon <- amazon %>% select(-one_of("Adj Close"))
amazon <- amazon %>% select(-one_of("Date"))
apple <- read_csv("datasets/AAPL.csv")
apple <- mutate(apple, Company="Apple")
apple$Date <- as.Date(apple$Date,format ='%m/%d/%Y' )
apple <- apple %>% select(-one_of("Adj Close"))
facebook <- read_csv("datasets/FB.csv")
facebook <- mutate(facebook, Company="Facebook")
facebook$Date <- as.Date(facebook$Date,format ='%m/%d/%Y' )
facebook <- facebook %>% select(-one_of("Adj Close"))
google <- read_csv("datasets/GOOG.csv")
google <- mutate(google, Company="Google")
google$Date <- as.Date(df$Date,format ='%m/%d/%Y' )
google <- google %>% select(-one_of("Adj Close"))
netflix <- read_csv("datasets/NFLX.csv")
netflix <- mutate(netflix, Company="Netflix")
netflix$Date <- as.Date(netflix$Date,format ='%m/%d/%Y' )
netflix <- netflix %>% select(-one_of("Adj Close"))
#Combining all the FAANG companies into one dataframe
FAANG <- rbind(amazon, apple, facebook, google, netflix)
#Removing Open, High, Low, and Adj Close column
FAANG <- FAANG %>% select(-one_of("Adj Close"))
FAANG$Date <- as.Date(FAANG$Date,format ='%m/%d/%Y' )
#Rearranging the dataframe so that Company comes first
FAANG <- select(FAANG, Company, Date, Open, High , Low, Close, Volume)
###############################################################################################
###############################################################################################
#shifting n rows up of a given variable
shift <- function(x, n) {
c(x[-(seq(n))], rep(NA, n))
}
amazon$shifted <- shift(amazon$Close, 1)
tail(amazon)
#remove NA observations
amazon <- na.omit(amazon)
write.csv(amazon, "amazon.csv")
#Initializing the Virtual Machine using all the threads (-1) and 16gb of memory
h2o.init(nthreads = -1, max_mem_size = "16g")
amazon <- h2o.importFile("amazon.csv")
h2o.describe(amazon)
y <- "shifted" #variable we want to forecast
x <- setdiff(names(amazon), y)
set.seed(1)
parts <- h2o.splitFrame(amazon, .80)
train <- parts[[1]]
test <- parts[[2]]
#Train the Model
automodel <- h2o.automl(x, y, train, test, max_runtime_secs = 120)
#Obtained a list of models in order of performance. To learn more about them just call
automodel@leader
#Apply the Mode
predictions <- h2o.predict(automodel@leader, test)
#Obtained a list of models in order of performance. To learn more about them just call
automodel@leader
clear
predictions
